#!/usr/bin/env python3
"""
Versiunea veche interfata facuta in gradio a modelului Ione. Modificarile au fost facute prin ssh direct in aplicatie. Versiunea finala se afla in RunPod. 

Ione AI Chat Assistant - RunPod Edition
This script creates a web-based chat interface for the IoneX8M AI model.
Run with: bash /workspace/startup.sh
"""

import os
import sys
import time
import argparse
import gradio as gr  # web UI framework
from pathlib import Path

# optimize PyTorch for better performance
os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
os.environ["TORCHDYNAMO_DISABLE"] = "1" 
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"

# try importing AI libraries
try:
    import torch
    import transformers
except ImportError:
    print("Installing required packages...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "torch transformers gradio"])
    import torch
    import transformers

# check for quantization library (saves memory)
try:
    import bitsandbytes as bnb
    have_bnb = True
    print("bitsandbytes is available - quantization enabled")
except ImportError:
    have_bnb = False
    print("bitsandbytes not found - will use standard precision")

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
from transformers.utils import logging

logging.set_verbosity_error()  # less spam in console

def load_ione_model(model_path, device="auto", use_quantization=True):
    """
    Load the AI model with memory optimization.
    Tries different compression methods if GPU memory is limited.
    """
    print(f"Loading IoneX8M model from {model_path}...")
    start_time = time.time()
    
    try:
        # load tokenizer (converts text to numbers)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # pick device automatically
        actual_device = device
        if device == "auto":
            if torch.cuda.is_available():
                actual_device = "cuda"
            else:
                actual_device = "cpu"
        
        # show GPU memory info
        if actual_device == "cuda":
            vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            vram_free = vram_total - (torch.cuda.memory_allocated(0) / 1024**3)
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"Total VRAM: {vram_total:.2f} GB, Free VRAM: {vram_free:.2f} GB")
        
        # try 4-bit compression first (saves 75% memory)
        if use_quantization and have_bnb and actual_device == "cuda":
            try:
                print("Using 4-bit quantization with optimized device mapping")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    llm_int8_threshold=6.0,
                    llm_int8_has_fp16_weight=True
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    quantization_config=quantization_config,
                    device_map="auto",  # auto-place model parts
                    torch_dtype=torch.float16,
                    offload_folder="offload",
                    offload_state_dict=True,
                    low_cpu_mem_usage=True
                )
            except Exception as e:
                # fallback to 8-bit (saves 50% memory)
                print(f"Error setting up 4-bit quantization: {e}")
                print("Falling back to 8-bit quantization")
                try:
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                        llm_int8_threshold=6.0,
                        llm_int8_has_fp16_weight=True
                    )
                    
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        quantization_config=quantization_config,
                        device_map="auto",
                        torch_dtype=torch.float16,
                        offload_folder="offload",
                        low_cpu_mem_usage=True
                    )
                except Exception as e2:
                    # last resort: no compression
                    print(f"Error setting up 8-bit quantization: {e2}")
                    print("Falling back to standard loading with device_map='auto'")
                    model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16 if actual_device == "cuda" else torch.float32,
                        device_map="auto",
                        offload_folder="offload",
                        low_cpu_mem_usage=True
                    )
        else:
            # no compression
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if actual_device == "cuda" else torch.float32,
                device_map="auto",
                offload_folder="offload",
                low_cpu_mem_usage=True
            )
        
        # create text generator
        generator = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device_map="auto"
        )
        
        load_time = time.time() - start_time
        print(f"IoneX8M model loaded successfully in {load_time:.2f} seconds!")
        
        return {"model": model, "tokenizer": tokenizer, "generator": generator}
    
    except Exception as e:
        print(f"Error loading IoneX8M model: {e}")
        raise

class ChatHistory:
    """Manages conversation history and formats it for the AI model"""
    
    def __init__(self, max_history=5, system_prompt=None):
        self.history = []
        self.max_history = max_history
        self.system_prompt = system_prompt or "You are Ione, a helpful AI assistant. Answer the user's questions thoroughly and accurately."
    
    def add_message(self, role, content):
        """Add new message and keep history under limit"""
        self.history.append({"role": role, "content": content})
        if len(self.history) > self.max_history * 2:  # user + assistant = 2 messages per turn
            self.history = self.history[-self.max_history * 2:]
    
    def format_prompt_ione(self):
        """Convert chat history to IoneX8M format with special tokens"""
        prompt = f"<|system|>\n{self.system_prompt}\n"
        
        for msg in self.history:
            if msg["role"] == "user":
                prompt += f"<|user|>\n{msg['content']}\n"
            elif msg["role"] == "assistant":
                prompt += f"<|assistant|>\n{msg['content']}\n"
        
        # add assistant token to prompt response
        if self.history and self.history[-1]["role"] == "user":
            prompt += "<|assistant|>\n"
            
        return prompt
    
    def clear(self):
        self.history = []
        
    def get_chatbot_history(self):
        """Format for Gradio chatbot component"""
        return [{"role": msg["role"], "content": msg["content"]} for msg in self.history]

class IoneAssistant:
    """Main AI assistant that handles chat interactions"""
    
    def __init__(self, model_path, device="auto", use_quantization=True):
        # pick device
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        # load model
        self.model_data = load_ione_model(model_path, self.device, use_quantization)
        self.chat_history = ChatHistory(max_history=10)
        
        # define AI personality
        self.system_prompt = """You are Ione, a versatile AI assistant powered by the IoneX8M model. You excel at both creative and technical tasks.

For coding and technical questions:
- Provide accurate, efficient, and well-documented code examples
- Explain complex technical concepts clearly
- Offer practical solutions to programming problems
- Be precise and thorough in your explanations

For creative tasks:
- Craft engaging and imaginative stories
- Provide thoughtful responses to open-ended questions
- Adapt your tone to match the user's needs
- Be conversational and natural in your interactions

As Ione, you aim to be helpful, accurate, and engaging in all your interactions."""
        
        print("Ione Chat Assistant (IoneX8M) initialized successfully!")
    
    def chat(self, user_message, max_length=500, temperature=0.7, top_p=0.9, top_k=40, continue_until_end=False):
        """Generate AI response to user message"""
        start_time = time.time()
        
        try:
            # free GPU memory
            if self.device == "cuda":
                torch.cuda.empty_cache()
            
            # add to conversation
            self.chat_history.add_message("user", user_message)
            self.chat_history.system_prompt = self.system_prompt
            prompt = self.chat_history.format_prompt_ione()
            
            # adjust response length based on GPU memory
            is_high_vram = False
            if self.device == "cuda":
                vram_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                is_high_vram = vram_total >= 16.0
            
            if is_high_vram:
                max_new_tokens = min(max_length * 2, 1024) if continue_until_end else min(max_length, 768)
            else:
                max_new_tokens = min(max_length, 512) if continue_until_end else min(max_length, 384)
            
            print(f"Generating with max_new_tokens: {max_new_tokens}")
            
            # generation settings
            generation_kwargs = {
                "do_sample": True,
                "temperature": temperature,  # higher = more creative
                "top_p": top_p,  # word selection range
                "top_k": top_k,  # word pool size
                "num_return_sequences": 1,
                "eos_token_id": self.model_data["tokenizer"].eos_token_id,
                "pad_token_id": self.model_data["tokenizer"].pad_token_id,
                "max_new_tokens": max_new_tokens,
                "repetition_penalty": 1.1,  # avoid repetition
                "no_repeat_ngram_size": 3,
                "use_cache": True
            }
            
            # generate response
            result = self.model_data["generator"](prompt, **generation_kwargs)
            generated_text = result[0]["generated_text"]
            
            # extract just the new response
            if "<|assistant|>" in generated_text:
                parts = generated_text.split("<|assistant|>")
                response = parts[-1].strip()
                
                # clean up response
                if "<|user|>" in response:
                    response = response.split("<|user|>")[0].strip()
                if "<|system|>" in response:
                    response = response.split("<|system|>")[0].strip()
                    
                # remove control tokens
                response = response.replace("<|start|>", "").replace("<|stop|>", "")
                response = response.replace("<|Start|>", "").replace("<|Stop|>", "")
            else:
                response = generated_text[len(prompt):].strip()
                response = response.replace("<|start|>", "").replace("<|stop|>", "")
                response = response.replace("<|Start|>", "").replace("<|Stop|>", "")
            
            # save response
            self.chat_history.add_message("assistant", response)
            
            # show performance stats
            gen_time = time.time() - start_time
            tokens_generated = len(response.split())
            print(f"Generated {tokens_generated} words in {gen_time:.2f} seconds")
            
            chatbot_history = self.chat_history.get_chatbot_history()
            return response, chatbot_history
        
        except RuntimeError as e:
            # handle GPU memory errors
            if "CUDA out of memory" in str(e):
                error_msg = "GPU ran out of memory. Try:\n- Reducing Max Length\n- Disabling 'Continue until end'\n\nError details: " + str(e)
                if self.device == "cuda":
                    torch.cuda.empty_cache()
            else:
                error_msg = str(e)
                
            self.chat_history.add_message("assistant", f"Error: {error_msg}")
            return f"Error: {error_msg}", self.chat_history.get_chatbot_history()
        
        except Exception as e:
            error_msg = str(e)
            self.chat_history.add_message("assistant", f"Error: {error_msg}")
            return f"Error: {error_msg}", self.chat_history.get_chatbot_history()
    
    def clear_chat(self):
        """Reset conversation"""
        self.chat_history.clear()
        return None, []

def create_chat_interface(model_path, device="auto", use_quantization=True, server_name=None, server_port=None, share=False):
    """Build the web interface"""
    # init AI
    ione = IoneAssistant(model_path, device, use_quantization)
    
    # conversation presets for different use cases
    presets = {
        "Balanced": {
            "temperature": 0.7, 
            "top_p": 0.9, 
            "top_k": 40, 
            "continue_until_end": False,
            "description": "Default setting with a good balance of creativity and accuracy."
        },
        "Creative": {
            "temperature": 1.0,  # more random
            "top_p": 0.95, 
            "top_k": 60, 
            "continue_until_end": False,
            "description": "More varied and imaginative responses, good for brainstorming ideas."
        },
        "Precise": {
            "temperature": 0.3,  # more focused
            "top_p": 0.85, 
            "top_k": 20, 
            "continue_until_end": False,
            "description": "More deterministic and focused responses for factual questions."
        },
        "Code": {
            "temperature": 0.2,  # very focused for code
            "top_p": 0.8, 
            "top_k": 15, 
            "continue_until_end": False,
            "description": "Optimized for generating accurate, functional code snippets."
        },
        "Creative Writing": {
            "temperature": 0.8, 
            "top_p": 0.92, 
            "top_k": 50, 
            "continue_until_end": True,  # complete thoughts
            "description": "Optimized for stories, essays and longer creative content."
        }
    }
    
    # dark theme styling
    custom_css = """
    .gradio-container {
        font-family: 'Inter', system-ui, -apple-system, BlinkMacSystemFont, sans-serif;
        max-width: 1200px;
        margin: auto;
        background-color: #0f0f1a;
        color: #f0f0f0;
    }
    .chat-container {
        height: 600px;
        border-radius: 12px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.3);
        background-color: #16161f;
    }
    .user-message {
        background-color: #29293a !important;
        border-radius: 12px;
        padding: 12px 16px;
        margin: 8px 0;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
        color: #ffffff;
    }
    .assistant-message {
        background-color: #38214d !important;
        border-radius: 12px;
        padding: 12px 16px;
        margin: 8px 0;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.2);
        color: #ffffff;
        border-left: 3px solid #ff3366;
    }
    .clear-btn {
        background-color: #38214d;
        color: white;
        border: none;
        border-radius: 8px;
        padding: 10px 16px;
        font-size: 0.95rem;
        font-weight: 500;
        transition: all 0.2s ease;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
    }
    .clear-btn:hover {
        background-color: #4b2a66;
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    }
    .send-btn {
        background-color: #ff3366;
        color: white;
        border: none;
        border-radius: 8px;
        padding: 10px 20px;
        font-size: 1rem;
        font-weight: 500;
        transition: all 0.2s ease;
        box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
    }
    .send-btn:hover {
        background-color: #ff477a;
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    }
    .memory-info {
        font-size: 0.85rem;
        color: #a3a3c2;
        text-align: right;
        padding: 6px 10px;
        background-color: #1e1e2d;
        border-radius: 6px;
        margin-top: 8px;
    }
    .footer {
        margin-top: 2rem;
        text-align: center;
        font-size: 0.9rem;
        color: #a3a3c2;
        padding: 15px;
        border-top: 1px solid #2d2d42;
    }
    h1 {
        color: #ffffff;
        font-weight: 600;
        margin-bottom: 20px;
        font-size: 32px;
    }
    .model-info {
        background-color: #2d1e3e;
        border-left: 4px solid #ff3366;
        padding: 12px 16px;
        margin: 12px 0 24px 0;
        font-size: 0.95rem;
        border-radius: 0 8px 8px 0;
        color: #e0e0e0;
    }
    .preset-description {
        font-size: 0.85rem;
        color: #a3a3c2;
        margin-top: 8px;
    }
    input, select, textarea {
        background-color: #1e1e2d !important;
        color: #ffffff !important;
        border: 1px solid #38214d !important;
    }
    .logo-image {
        width: 32px;
        height: 32px;
        margin-right: 10px;
        vertical-align: middle;
    }
    .title-with-logo {
        display: flex;
        align-items: center;
    }
    """
    
    # build interface
    with gr.Blocks(title="Ione AI Chat Assistant", css=custom_css) as interface:
        
        # header
        with gr.Row():
            gr.HTML("""
            <div class="title-with-logo">
                <img src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyMDAgMjAwIj48ZGVmcz48c3R5bGU+LmlyaXMtZ3JhZGllbnR7ZmlsbDp1cmwoI2lyaXMtZ3JhZGllbnQpfS5wdXBpbHtmaWxsOiMwMDB9LmN1cnNvcntmaWxsOiNmZjMzNjZ9PC9zdHlsZT48cmFkaWFsR3JhZGllbnQgaWQ9ImlyaXMtZ3JhZGllbnQiIGN4PSIxMDAiIGN5PSIxMDAiIHI9IjgwIiBncmFkaWVudFVuaXRzPSJ1c2VyU3BhY2VPblVzZSI+PHN0b3Agb2Zmc2V0PSIwIiBzdG9wLWNvbG9yPSIjOTgwMDk4Ii8+PHN0b3Agb2Zmc2V0PSIuNSIgc3RvcC1jb2xvcj0iI2ZmMzM2NiIvPjxzdG9wIG9mZnNldD0iMSIgc3RvcC1jb2xvcj0iIzk4MDA5OCIvPjwvcmFkaWFsR3JhZGllbnQ+PC9kZWZzPjxjaXJjbGUgY3g9IjEwMCIgY3k9IjEwMCIgcj0iOTciIGNsYXNzPSJpcmlzLWdyYWRpZW50Ii8+PGNpcmNsZSBjeD0iMTAwIiBjeT0iMTAwIiByPSIzMCIgY2xhc3M9InB1cGlsIi8+PHBhdGggY2xhc3M9ImN1cnNvciIgZD0iTTUwIDEwMHMyMC0zNSA1MC0zNSA1MCAzNSA1MCAzNWwzMC0yMHYtMjBsLTMwIDIwUzEzMCAzMSA2NSAzMSA1IDEwMCA1IDEwMEwyMCA4MHYyNXoiLz48L3N2Zz4=" class="logo-image" alt="Ione AI Logo">
                <h1>Ione AI Chat Assistant</h1>
            </div>
            """)
        
        # model info
        gr.Markdown(
            """<div class="model-info">
            <strong>Powered by IoneX8M</strong> - An advanced AI model for creative writing, conversation, and technical tasks including coding.
            </div>""")
            
        # main layout: chat (75%) + controls (25%)
        with gr.Row():
            with gr.Column(scale=3):  # chat area
                
                # conversation display
                chatbot = gr.Chatbot(
                    label="Conversation",
                    elem_classes=["chat-container"],
                    height=500,
                    type="messages"
                )
                
                # input area
                with gr.Row():
                    user_input = gr.Textbox(
                        placeholder="Type your message here...",
                        label="Your Message",
                        lines=3,
                        show_label=False
                    )
                    submit_btn = gr.Button("Send", elem_classes=["send-btn"])
                
                clear_btn = gr.Button("Clear Conversation", elem_classes=["clear-btn"])
                memory_info = gr.Markdown("", elem_classes=["memory-info"])  # GPU memory display
            
            with gr.Column(scale=1):  # controls
                
                # preset selection
                gr.Markdown("### Conversation Style")
                
                preset_selector = gr.Radio(
                    choices=list(presets.keys()),
                    value="Balanced",
                    label="Select a conversation style:"
                )
                
                preset_description = gr.Markdown(
                    value=f"*{presets['Balanced']['description']}*",
                    elem_classes=["preset-description"]
                )
                
                # response length
                max_length = gr.Slider(
                    minimum=100,
                    maximum=2000,
                    value=750,
                    step=50,
                    label="Max Response Length"
                )
                
                # advanced controls (hidden by default)
                with gr.Accordion("Advanced Settings", open=False):
                    
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature"
                    )
                    
                    top_p = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.9,
                        step=0.05,
                        label="Top-p"
                    )
                    
                    top_k = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=40,
                        step=1,
                        label="Top-k"
                    )
                    
                    continue_generation = gr.Checkbox(
                        label="Continue until end",
                        value=False,
                        info="Try to complete the response naturally rather than cutting off at max length"
                    )
        
        # event handlers
        def chat_with_ione(user_message, chatbot_history, max_length, temperature, top_p, top_k, continue_until_end):
            """Handle user input and generate response"""
            if not user_message.strip():
                return "", chatbot_history, update_memory_info()
            
            # get AI response
            _, updated_chatbot = ione.chat(
                user_message=user_message,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                continue_until_end=continue_until_end
            )
            
            memory_status = update_memory_info()
            return "", updated_chatbot, memory_status  # clear input, update chat, show memory
        
        def apply_preset(preset_name):
            """Change all sliders when preset is selected"""
            preset = presets[preset_name]
            description = f"*{preset['description']}*"
            return [
                preset["temperature"],
                preset["top_p"],
                preset["top_k"],
                preset["continue_until_end"],
                description
            ]
        
        def update_memory_info():
            """Show current GPU memory usage"""
            if torch.cuda.is_available():
                vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                vram_free = vram_total - (torch.cuda.memory_allocated(0) / 1024**3)
                return f"GPU Memory: {vram_free:.2f}GB free of {vram_total:.2f}GB total"
            return "Running on CPU"
        
        # connect buttons to functions
        submit_btn.click(
            fn=chat_with_ione,
            inputs=[user_input, chatbot, max_length, temperature, top_p, top_k, continue_generation],
            outputs=[user_input, chatbot, memory_info]
        )
        
        # also send on Enter key
        user_input.submit(
            fn=chat_with_ione,
            inputs=[user_input, chatbot, max_length, temperature, top_p, top_k, continue_generation],
            outputs=[user_input, chatbot, memory_info]
        )
        
        # clear button
        clear_btn.click(
            fn=ione.clear_chat,
            inputs=[],
            outputs=[user_input, chatbot]
        )
        
        # preset changes
        preset_selector.change(
            fn=apply_preset,
            inputs=preset_selector,
            outputs=[temperature, top_p, top_k, continue_generation, preset_description]
        )
        
        # show memory on load
        interface.load(fn=update_memory_info, inputs=None, outputs=memory_info)
        
        # footer
        gr.Markdown(
            """<div class="footer">
                <strong>Ione AI Chat Assistant</strong> - Powered by IoneX8M<br>
                An optimized conversational interface for local AI interactions
            </div>""", 
            elem_classes=["footer"]
        )
    
    # start web server
    interface.launch(
        server_name=server_name,
        server_port=server_port,
        share=share
    )

def main():
    """Entry point - handle command line arguments and start app"""
    parser = argparse.ArgumentParser(description="Ione AI Chat Assistant (RunPod Edition)")
    
    # command line options
    parser.add_argument("--model-path", type=str, default="./ionex8m-model", 
                        help="Where to find IoneX8M model files")
    parser.add_argument("--device", type=str, choices=["auto", "cuda", "cpu"], default="auto", 
                        help="Which processor to use - auto picks GPU if available")
    parser.add_argument("--no-quantization", action="store_true", 
                        help="Don't compress model (uses more memory)")
    parser.add_argument("--server-name", type=str, default="0.0.0.0", 
                        help="IP address for web server")
    parser.add_argument("--server-port", type=int, default=7860, 
                        help="Port number for web interface")
    parser.add_argument("--share", action="store_true", 
                        help="Create public URL")
    
    args = parser.parse_args()
    
    # startup banner
    print("\n" + "="*60)
    print("              Ione AI Chat Assistant")
    print("="*60 + "\n")
    
    # check model exists
    if not os.path.exists(args.model_path):
        print(f"Error: Model path '{args.model_path}' does not exist")
        print("Make sure you downloaded IoneX8M model to correct folder!")
        return
    
    # start interface
    create_chat_interface(
        model_path=args.model_path,
        device=args.device,
        use_quantization=not args.no_quantization,
        server_name=args.server_name,
        server_port=args.server_port,
        share=args.share
    )

if __name__ == "__main__":
    main()
